{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.models import VGG19_Weights, vgg19\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "# ClearML for experement tracking (optional)\n",
    "from clearml import Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set device to GPU if available, else CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\": \n",
    "    print(\"Running on GPU\")\n",
    "else:\n",
    "    print(\"Running on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set path to dataset images directory and change working directory to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"PATH TO IMAAGE DIRECTORY\"\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of image samples file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "with os.scandir(path) as files:\n",
    "    for file in files:\n",
    "        if file.name.endswith('.png'):\n",
    "            samples.append(file.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load VGG19 model with pre-trained weights and remove classifier layers after first layer (change if you want to experement with other models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg19(weights=VGG19_Weights.IMAGENET1K_V1)\n",
    "model.classifier = model.classifier[:1]\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image transformer initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set image transformation parameters\n",
    "resize_size = 256\n",
    "crop_size = 224\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define image transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(resize_size, interpolation=InterpolationMode.BILINEAR),\n",
    "    transforms.CenterCrop(crop_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file, model, transform):\n",
    "    \"\"\"\n",
    "    Extract features from an image using a pre-trained model.\n",
    "    \n",
    "    Args:\n",
    "        file (str): Path to image file.\n",
    "        model (torch.nn.Module): Pre-trained model for feature extraction.\n",
    "        transform (torchvision.transforms.Compose): Image transformation pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Extracted features.\n",
    "    \"\"\"\n",
    "    img = Image.open(file)\n",
    "    img = img.convert('RGB')\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    features = model(img)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features from sample images and save feature vectors to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from sample images and save feature vectors to a dictionary\n",
    "data = {}\n",
    "p = r\"CHANGE TO A LOCATION TO SAVE FEATURE VECTORS\"\n",
    "\n",
    "for sample in samples:\n",
    "    try:\n",
    "        feat = extract_features(sample, model, transform)\n",
    "        data[sample] = feat.detach().cpu().numpy()\n",
    "    except:\n",
    "        with open(p,'wb') as file:\n",
    "            pickle.dump(data,file)\n",
    "            \n",
    "# Convert dictionary of feature vectors to numpy arrays for clustering\n",
    "filenames = np.array(list(data.keys()))\n",
    "feat = np.array(list(data.values()))\n",
    "feat = feat.reshape(-1, 4096) # Change to the number of features (if you changed the model)\n",
    "# The output tensor should have the shape (number of samples, length of image feature vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = randint(0, 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experementation (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experementation(feat, random_state):\n",
    "    \"\"\"\n",
    "    Experiment with different PCA and KMeans parameters to find optimal clustering.\n",
    "    \n",
    "    Args:\n",
    "        feat (np.ndarray): Feature vectors for clustering.\n",
    "    \"\"\"\n",
    "    for n_components_loop in range(1, feat.shape[0]):\n",
    "        if n_components_loop != 1 and  n_components_loop % 5 != 0:\n",
    "            continue\n",
    "        \n",
    "        pca = PCA(n_components=n_components_loop, random_state=random_state,)\n",
    "        pca.fit(feat)\n",
    "        x = pca.transform(feat)\n",
    "    \n",
    "        task = Task.init(\n",
    "            project_name='Deep clustering',\n",
    "            task_name=f'VGG19 PCA {n_components_loop} randomized',\n",
    "            tags=['k-mean','VGG19','randomized'],\n",
    "            output_uri=None\n",
    "        )\n",
    "\n",
    "        logger = task.get_logger()\n",
    "            \n",
    "        sse = []\n",
    "\n",
    "        for n_clusters in range(2, feat.shape[0]):\n",
    "\n",
    "            kmeans = KMeans(n_clusters=n_clusters, init = 'k-means++', n_init='auto', random_state=random_state)\n",
    "            kmeans.fit(x)\n",
    "            labels = kmeans.labels_\n",
    "                \n",
    "            silhouette = silhouette_score(x, labels)\n",
    "            calinski_harabasz = calinski_harabasz_score(x, labels)\n",
    "            davies_bouldin = davies_bouldin_score(x, labels)\n",
    "                \n",
    "            logger.report_scalar(title='Silhouette Score', series='Scores', value=silhouette, iteration=n_clusters)\n",
    "            logger.report_scalar(title='Calinski-Harabasz Index', series='Scores', value=calinski_harabasz, iteration=n_clusters)\n",
    "            logger.report_scalar(title='Davies-Bouldin Index', series='Scores', value=davies_bouldin, iteration=n_clusters)\n",
    "\n",
    "\n",
    "            sse.append(kmeans.inertia_)\n",
    "\n",
    "        fig = px.scatter(x=range(2, feat.shape[0]), y=sse)\n",
    "\n",
    "        fig.update_traces(mode='lines+markers')\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=\"Sum of Squared Distance vs Number of Clusters\",\n",
    "            xaxis_title=\"Number of clusters (k)\",\n",
    "            yaxis_title=\"Sum of squared distance\"\n",
    "        )\n",
    "            \n",
    "        logger.report_plotly(title=f'Sum of Squared Distance vs Number of Clusters PCA = {n_components_loop}', series='Scores k-means++', figure=fig)\n",
    "            \n",
    "        task.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you want to run the experementation\n",
    "# experementation(feat, random_state) \n",
    "# Check the results and pick optimal hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set PCA and KMeans parameters and fit models to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 200\n",
    "num_of_clusters = 10\n",
    "\n",
    "f\"Random state: {random_state}\"\n",
    "unique_labels = [i for i in range(num_of_clusters)]\n",
    "\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized', random_state=random_state)\n",
    "pca.fit(feat)\n",
    "x = pca.transform(feat)\n",
    "\n",
    "kmeans = KMeans(n_clusters=len(unique_labels), init='k-means++', n_init='auto', random_state=random_state)\n",
    "kmeans.fit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot elbow curve to determine optimal number of clusters for KMeans algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "list_k = list(range(2, feat.shape[0]))\n",
    "\n",
    "for k in list_k:\n",
    "    km = KMeans(n_clusters=k, init='k-means++', n_init='auto')\n",
    "    km.fit(x)\n",
    "    sse.append(km.inertia_)\n",
    "\n",
    "fig = px.line(x=list_k, y=sse)\n",
    "fig.update_layout(\n",
    "    title='Elbow Curve',\n",
    "    xaxis_title=r'Number of clusters *k*',\n",
    "    yaxis_title='Sum of squared distance',\n",
    "    width=2000,\n",
    "    height=800\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize clustering results in 1D, 2D or 3D depending on number of PCA components used for clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_components == 1:\n",
    "    pca_1d = PCA(n_components=1, random_state=random_state)\n",
    "    x_1d = pca_1d.fit_transform(x)\n",
    "\n",
    "    df = pd.DataFrame(x_1d, columns=['x'])\n",
    "    df['cluster'] = kmeans.labels_\n",
    "\n",
    "    fig = px.scatter(df, x='x', y='cluster', title='KMeans Clustering')\n",
    "    fig.show()\n",
    "\n",
    "elif n_components == 2:\n",
    "    pca_2d = PCA(n_components=2, random_state=random_state)\n",
    "    x_2d = pca_2d.fit_transform(x)\n",
    "\n",
    "    df = pd.DataFrame(x_2d, columns=['x', 'y'])\n",
    "    df['cluster'] = kmeans.labels_\n",
    "\n",
    "    fig = px.scatter(df, x='x', y='y', color='cluster', title='KMeans Clustering')\n",
    "    fig.show()\n",
    "\n",
    "elif n_components == 3:\n",
    "    pca_3d = PCA(n_components=3, random_state=random_state)\n",
    "    x_3d = pca_3d.fit_transform(x)\n",
    "\n",
    "    df = pd.DataFrame(x_3d, columns=['x', 'y', 'z'])\n",
    "    df['cluster'] = kmeans.labels_\n",
    "\n",
    "    fig = px.scatter_3d(df, x='x', y='y', z='z', color='cluster', title='KMeans Clustering')\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Can't visualize more than 3 dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group sample images by cluster assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = {}\n",
    "for file, cluster in zip(filenames, kmeans.labels_):\n",
    "    if cluster not in groups.keys():\n",
    "        groups[cluster] = []\n",
    "        groups[cluster].append(file)\n",
    "    else:\n",
    "        groups[cluster].append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize all clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_cluster(cluster):\n",
    "    \"\"\"\n",
    "    Visualize a cluster of sample images.\n",
    "    \n",
    "    Args:\n",
    "        cluster (int): Cluster number.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(25, 25))\n",
    "    files = groups[cluster]\n",
    "    if len(files) > 30:\n",
    "        print(f\"Clipping cluster size from {len(files)} to 30\")\n",
    "        files = files[:29]\n",
    "    for index, file in enumerate(files):\n",
    "        plt.subplot(10, 10, index + 1)\n",
    "        img = Image.open(file).convert('RGB')\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_of_clusters):\n",
    "    view_cluster(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
